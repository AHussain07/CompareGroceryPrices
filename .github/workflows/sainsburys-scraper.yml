name: Sainsbury's Price Scraper

on:
  workflow_dispatch:
  schedule:
    - cron: '0 0 * * 4'  # Wednesdays at 12:00 AM UTC (midnight)

permissions:
  contents: write

jobs:
  scrape-sainsburys:
    runs-on: ubuntu-latest
    timeout-minutes: 150
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wget curl unzip xvfb
        sudo apt-get install -y libnss3-dev libatk-bridge2.0-0 libdrm2 libxkbcommon0 libgtk-3-0 libatspi2.0-0
        sudo apt-get install -y libasound2t64 libxrandr2 libpangocairo-1.0-0 libatk1.0-0 libcairo-gobject2 libgdk-pixbuf2.0-0
    
    - name: Install Chrome and matching ChromeDriver
      run: |
        # Get actual Chrome version first
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        
        # Get the ACTUAL Chrome version
        CHROME_VERSION=$(google-chrome --version | grep -oP '\d+\.\d+\.\d+\.\d+')
        CHROME_MAJOR=$(echo $CHROME_VERSION | cut -d. -f1)
        echo "Actual Chrome version: $CHROME_VERSION (major: $CHROME_MAJOR)"
        
        # Use correct ChromeDriver versions that actually exist
        if [ "$CHROME_MAJOR" = "140" ]; then
          CHROMEDRIVER_VERSION="140.0.7339.80"  # This version exists
        elif [ "$CHROME_MAJOR" = "139" ]; then
          CHROMEDRIVER_VERSION="139.0.6858.80"
        elif [ "$CHROME_MAJOR" = "129" ]; then
          CHROMEDRIVER_VERSION="129.0.6668.89"
        else
          # Universal fallback to a stable version
          CHROMEDRIVER_VERSION="129.0.6668.89"
        fi
        
        echo "Using ChromeDriver version: $CHROMEDRIVER_VERSION"
        
        # Try downloading ChromeDriver
        CHROMEDRIVER_URL="https://storage.googleapis.com/chrome-for-testing-public/$CHROMEDRIVER_VERSION/linux64/chromedriver-linux64.zip"
        echo "Downloading from: $CHROMEDRIVER_URL"
        
        if wget -O /tmp/chromedriver.zip "$CHROMEDRIVER_URL"; then
          echo "‚úÖ ChromeDriver downloaded successfully"
          sudo unzip /tmp/chromedriver.zip -d /tmp/
          sudo mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver
          sudo chmod +x /usr/local/bin/chromedriver
        else
          echo "‚ùå Failed to download ChromeDriver version $CHROMEDRIVER_VERSION"
          echo "üîÑ Trying fallback version 129.0.6668.89..."
          
          # Fallback to known working version
          wget -O /tmp/chromedriver.zip "https://storage.googleapis.com/chrome-for-testing-public/129.0.6668.89/linux64/chromedriver-linux64.zip"
          sudo unzip /tmp/chromedriver.zip -d /tmp/
          sudo mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver
          sudo chmod +x /usr/local/bin/chromedriver
          echo "‚úÖ Fallback ChromeDriver installed"
        fi
        
        # Verify installation
        echo "Chrome version: $(google-chrome --version)"
        echo "ChromeDriver version: $(chromedriver --version)"
        echo "ChromeDriver location: $(which chromedriver)"
    
    - name: Install Python dependencies
      run: |
        cd WebScrape
        pip install --upgrade pip
        pip install -r requirements.txt
        
        echo "Installed packages:"
        pip list | grep -E "(selenium|pandas|undetected|psutil|beautifulsoup|lxml|requests)"
    
    - name: Create public directory
      run: mkdir -p app/public
    
    - name: Run Sainsbury's scraper with progress
      timeout-minutes: 140  # Increased from 115 to 140 minutes
      run: |
        cd WebScrape
        echo "üõí Starting Enhanced Sainsbury's scraper..."
        echo "üìã Mode: Enhanced with anti-detection & real-time progress"
        echo "üîß Sequential processing (MAX_THREADS = 1)"
        echo "üìÑ All categories included"
        echo "üõ°Ô∏è Anti-bot measures: Random delays, blocking detection, user-agent rotation"
        echo "üéØ Target: 10,000+ products (minimum for commit)"
        echo "üïê Started: $(date)"
        
        # Set display for virtual framebuffer
        export DISPLAY=:99
        
        # Start virtual display
        Xvfb :99 -screen 0 1920x1080x24 > /dev/null 2>&1 &
        sleep 5
        
        # Run with unbuffered output for real-time progress
        echo "Executing: python -u sainsburys.py"
        timeout 8400 python -u sainsburys.py 2>&1 | tee sainsburys.log
        
        echo "Enhanced Sainsbury's scraper completed at: $(date)"
    
    - name: Check results and copy CSV
      id: results
      run: |
        cd WebScrape
        
        echo "Checking for output files..."
        ls -la *.csv 2>/dev/null || echo "No CSV files found"
        
        if [ -f "sainsburys.csv" ]; then
          echo "‚úÖ Found sainsburys.csv"
          
          # Copy to public directory regardless of count (for analysis)
          cp sainsburys.csv ../app/public/
          echo "‚úÖ Copied to app/public/"
          
          # Get detailed stats
          total_lines=$(wc -l < sainsburys.csv)
          product_count=$((total_lines - 1))
          file_size=$(ls -lh sainsburys.csv | awk '{print $5}')
          
          echo "üìä Results:"
          echo "- Products: $product_count"
          echo "- File size: $file_size"
          echo "- Total lines: $total_lines"
          echo "- Minimum required: 10,000 products"
          
          # Set outputs for all cases
          echo "product_count=$product_count" >> $GITHUB_OUTPUT
          
          # Check if we meet the minimum threshold for commit
          if [ $product_count -ge 10000 ]; then
            echo "‚úÖ SUCCESS: $product_count products ‚â• 10,000 minimum threshold"
            echo "‚úÖ This data will be committed to the repository"
            echo "success=true" >> $GITHUB_OUTPUT
            echo "should_commit=true" >> $GITHUB_OUTPUT
            
            # Show sample products
            echo ""
            echo "üìã Sample products (first 5):"
            head -6 sainsburys.csv | tail -5
            
            # Show category breakdown
            echo ""
            echo "üìä Categories with products:"
            tail -n +2 sainsburys.csv | cut -d',' -f1 | sort | uniq -c | sort -nr | head -10
            
          elif [ $product_count -gt 0 ]; then
            echo "‚ö†Ô∏è INSUFFICIENT: $product_count products < 10,000 minimum threshold"
            echo "‚ö†Ô∏è Data will NOT be committed (likely bot detection occurred)"
            echo "‚ö†Ô∏è Manual investigation recommended - check logs for blocking indicators"
            echo "success=false" >> $GITHUB_OUTPUT
            echo "should_commit=false" >> $GITHUB_OUTPUT
            
            # Still show some analysis for debugging
            echo ""
            echo "üìã Sample of what was scraped (first 5):"
            head -6 sainsburys.csv | tail -5
            
            echo ""
            echo "üìä Categories found:"
            tail -n +2 sainsburys.csv | cut -d',' -f1 | sort | uniq -c | sort -nr | head -5
            
          else
            echo "‚ùå CSV file created but contains 0 products"
            echo "success=false" >> $GITHUB_OUTPUT
            echo "should_commit=false" >> $GITHUB_OUTPUT
          fi
          
          # Show performance stats from log
          echo ""
          echo "üìä Performance stats:"
          grep -E "Total products:|Time:|products/sec|WARNING|SUCCESS" sainsburys.log | tail -10
          
        else
          echo "‚ùå No sainsburys.csv found"
          echo ""
          echo "üìã Directory contents:"
          ls -la
          echo ""
          echo "üìã Last 50 lines of log:"
          tail -50 sainsburys.log 2>/dev/null || echo "No log file"
          
          echo "success=false" >> $GITHUB_OUTPUT
          echo "should_commit=false" >> $GITHUB_OUTPUT
          echo "product_count=0" >> $GITHUB_OUTPUT
        fi
    
    - name: Upload logs and backup
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: sainsburys-enhanced-logs-${{ github.run_number }}
        path: |
          WebScrape/sainsburys.log
          WebScrape/sainsburys.py
          app/public/sainsburys.csv
        retention-days: 30
        if-no-files-found: ignore
    
    - name: Commit changes if successful and meets threshold
      if: steps.results.outputs.should_commit == 'true' && steps.results.outputs.product_count >= 10000
      run: |
        git config --global user.email "actions@github.com"
        git config --global user.name "GitHub Actions - Enhanced Sainsbury's"
        
        # Add CSV file
        git add -f app/public/sainsburys.csv
        
        if ! git diff --staged --quiet; then
          echo "üìù Committing changes with ${{ steps.results.outputs.product_count }} products..."
          
          git commit -m "üõí Sainsbury's price update (Enhanced Anti-Detection) - $(date -u '+%Y-%m-%d %H:%M UTC')

          ‚úÖ Products: ${{ steps.results.outputs.product_count }} (‚â• 10,000 threshold)
          üõ°Ô∏è Store: Sainsbury's (Enhanced Script)
          ü§ñ Anti-Detection: Blocking detection, random delays, user-agent rotation
          üìä Processing: Sequential (MAX_THREADS = 1)
          üìã Categories: All included with randomized order
          
          Enhanced Features:
          - Real-time blocking detection
          - Random delays and human-like behavior
          - User-agent rotation and stealth options
          - Category shuffling to avoid patterns
          - Quality threshold enforcement (10K+ products)
          
          Auto-updated via GitHub Actions"
          
          git push origin HEAD:${{ github.ref_name }}
          echo "‚úÖ Changes pushed successfully with ${{ steps.results.outputs.product_count }} products"
        else
          echo "‚ÑπÔ∏è No changes to commit"
        fi
    
    - name: Log insufficient data warning
      if: steps.results.outputs.product_count > 0 && steps.results.outputs.product_count < 10000
      run: |
        echo "‚ö†Ô∏è WARNING: Insufficient data quality detected"
        echo "- Products found: ${{ steps.results.outputs.product_count }}"
        echo "- Required minimum: 10,000"
        echo "- Likely cause: Bot detection by Sainsbury's website"
        echo "- Recommendation: Check logs and consider manual investigation"
        echo ""
        echo "This indicates the enhanced anti-detection measures may need further improvement"
        echo "or that Sainsbury's has updated their bot detection systems."
    
    - name: Summary
      if: always()
      run: |
        echo "## üõí Enhanced Sainsbury's Scraper Summary" >> $GITHUB_STEP_SUMMARY
        echo "**Run completed:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.results.outputs.should_commit }}" == "true" ]; then
          echo "### ‚úÖ Success - Data Committed" >> $GITHUB_STEP_SUMMARY
          echo "- **Products scraped:** ${{ steps.results.outputs.product_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Quality threshold:** ‚úÖ Met (‚â• 10,000 products)" >> $GITHUB_STEP_SUMMARY
          echo "- **Script:** Enhanced with anti-detection measures" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ‚úÖ Data committed to repository" >> $GITHUB_STEP_SUMMARY
          
        elif [ "${{ steps.results.outputs.product_count }}" -gt "0" ] && [ "${{ steps.results.outputs.product_count }}" -lt "10000" ]; then
          echo "### ‚ö†Ô∏è Insufficient Data Quality" >> $GITHUB_STEP_SUMMARY
          echo "- **Products found:** ${{ steps.results.outputs.product_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Quality threshold:** ‚ùå Not met (< 10,000 products)" >> $GITHUB_STEP_SUMMARY
          echo "- **Likely cause:** Bot detection by Sainsbury's" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ‚ùå Data NOT committed" >> $GITHUB_STEP_SUMMARY
          echo "- **Action needed:** Manual investigation recommended" >> $GITHUB_STEP_SUMMARY
          
        else
          echo "### ‚ùå Scraping Failed" >> $GITHUB_STEP_SUMMARY
          echo "- **Error:** Failed to generate usable CSV file" >> $GITHUB_STEP_SUMMARY
          echo "- **Products:** 0" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** ‚ùå Complete failure" >> $GITHUB_STEP_SUMMARY
          echo "- **Action needed:** Check logs for technical issues" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üõ°Ô∏è Enhanced Anti-Detection Features" >> $GITHUB_STEP_SUMMARY
        echo "- **Blocking Detection:** Real-time detection of CAPTCHAs and soft blocks" >> $GITHUB_STEP_SUMMARY
        echo "- **Random Behavior:** Variable delays, shuffled categories, human-like scrolling" >> $GITHUB_STEP_SUMMARY
        echo "- **Stealth Options:** User-agent rotation, additional Chrome stealth flags" >> $GITHUB_STEP_SUMMARY
        echo "- **Quality Control:** 10,000+ product threshold before committing data" >> $GITHUB_STEP_SUMMARY
        echo "- **Early Detection:** Monitoring for patterns indicating bot detection" >> $GITHUB_STEP_SUMMARY
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### üìã Technical Details" >> $GITHUB_STEP_SUMMARY
        echo "- **Driver:** Undetected ChromeDriver with enhanced stealth" >> $GITHUB_STEP_SUMMARY
        echo "- **Mode:** Headless (GitHub Actions)" >> $GITHUB_STEP_SUMMARY
        echo "- **Processing:** Sequential with randomized category order" >> $GITHUB_STEP_SUMMARY
        echo "- **Timeout:** 2.3 hours total" >> $GITHUB_STEP_SUMMARY
        echo "- **Retention:** Logs kept for 30 days" >> $GITHUB_STEP_SUMMARY