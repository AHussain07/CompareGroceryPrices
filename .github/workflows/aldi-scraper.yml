name: ALDI Price Scraper

on:
  workflow_dispatch:
  schedule:
    - cron: '0 7 * * 4' #Thursdays at 7:00 AM UTC

permissions:
  contents: write
  pages: write
  id-token: write

jobs:
  scrape-aldi:
    runs-on: ubuntu-latest
    timeout-minutes: 30
    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wget curl unzip xvfb
    
    - name: Install Chrome
      run: |
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
    
    - name: Install ChromeDriver
      run: |
        CHROME_VERSION=$(google-chrome --version | sed 's/Google Chrome //' | cut -d'.' -f1)
        
        if [ "$CHROME_VERSION" -ge "115" ]; then
          LATEST_VERSION=$(curl -s "https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_STABLE")
          wget -O /tmp/chromedriver.zip "https://storage.googleapis.com/chrome-for-testing-public/$LATEST_VERSION/linux64/chromedriver-linux64.zip"
          sudo unzip /tmp/chromedriver.zip -d /tmp/
          sudo mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver
        else
          DRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION}")
          wget -O /tmp/chromedriver.zip "https://chromedriver.storage.googleapis.com/${DRIVER_VERSION}/chromedriver_linux64.zip"
          sudo unzip /tmp/chromedriver.zip -d /usr/local/bin/
        fi
        
        sudo chmod +x /usr/local/bin/chromedriver
    
    - name: Install dependencies
      run: |
        cd WebScrape
        pip install -r requirements.txt
    
    - name: Fix ALDI scraper for GitHub Actions
      run: |
        cd WebScrape
        cat > fix_aldi.py << 'EOF'
        import re
        
        # Read the current aldi.py file
        with open('aldi.py', 'r') as f:
            content = f.read()
        
        # Enable headless mode
        content = re.sub(r'# (options\.add_argument\(\'--headless\'\))', r'\1', content)
        
        # Add additional headless options
        if '--no-sandbox' not in content:
            content = content.replace(
                'options.add_argument(\'--headless\')',
                '''options.add_argument('--headless')
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')'''
            )
        
        # Fix any indentation issues
        lines = content.split('\n')
        fixed_lines = []
        for line in lines:
            # Fix common indentation issues
            if 'options.add_argument(' in line and not line.strip().startswith('#'):
                if not line.startswith('    ') and not line.startswith('\t'):
                    line = '    ' + line.strip()
            fixed_lines.append(line)
        
        content = '\n'.join(fixed_lines)
        
        # Write the fixed file
        with open('aldi.py', 'w') as f:
            f.write(content)
        
        print("ALDI scraper fixed for headless mode")
        EOF
        
        python fix_aldi.py
    
    - name: Create public directory
      run: mkdir -p app/public
    
    - name: Run ALDI scraper with virtual display
      id: scraping
      run: |
        cd WebScrape
        echo "Starting ALDI scraper..."
        xvfb-run -a -s "-screen 0 1920x1080x24" python aldi.py
    
    - name: Check results and copy CSV
      id: results
      run: |
        echo "Checking for ALDI CSV files..."
        
        # Check in WebScrape directory
        if [ -f "WebScrape/aldi.csv" ]; then
          echo "Found aldi.csv in WebScrape directory"
          cp WebScrape/aldi.csv app/public/aldi.csv
        fi
        
        # Check if already in app/public
        if [ -f "app/public/aldi.csv" ]; then
          lines=$(wc -l < app/public/aldi.csv)
          product_count=$((lines - 1))
          echo "âœ… ALDI CSV created successfully with $lines lines ($product_count products)"
          echo "success=true" >> $GITHUB_OUTPUT
          echo "product_count=$product_count" >> $GITHUB_OUTPUT
        else
          echo "âŒ ALDI CSV not found in either location"
          echo "Contents of WebScrape directory:"
          ls -la WebScrape/
          echo "Contents of app/public directory:"
          ls -la app/public/
          echo "success=false" >> $GITHUB_OUTPUT
          echo "product_count=0" >> $GITHUB_OUTPUT
          exit 1
        fi
    
    - name: Commit and push changes
      if: steps.results.outputs.success == 'true'
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action - ALDI Scraper"
        
        # Force add the CSV file (in case .gitignore blocks it)
        git add -f app/public/aldi.csv
        
        if ! git diff --staged --quiet; then
          git commit -m "ðŸ›’ ALDI price update - $(date -u '+%Y-%m-%d %H:%M UTC')

          Products: ${{ steps.results.outputs.product_count }}
          Store: ALDI
          
          Auto-updated via GitHub Actions"
          git push
          echo "âœ… Changes committed and pushed"
        else
          echo "â„¹ï¸ No changes to commit"
        fi
    
    - name: Setup Node.js for website rebuild
      if: steps.results.outputs.success == 'true'
      uses: actions/setup-node@v4
      with:
        node-version: '18'
        cache: 'npm'
        cache-dependency-path: app/package-lock.json
    
    - name: Install dependencies and build website
      if: steps.results.outputs.success == 'true'
      run: |
        cd app
        echo "ðŸ“¦ Installing dependencies..."
        npm ci
        
        echo "ðŸ”¨ Building website with updated prices..."
        npm run build
        
        echo "âœ… Website built successfully"
        echo "Build directory contents:"
        ls -la build/
    
    - name: Setup Pages
      if: steps.results.outputs.success == 'true'
      uses: actions/configure-pages@v4
    
    - name: Upload to GitHub Pages
      if: steps.results.outputs.success == 'true'
      uses: actions/upload-pages-artifact@v3
      with:
        path: app/build
    
    - name: Deploy to GitHub Pages
      if: steps.results.outputs.success == 'true'
      id: deployment
      uses: actions/deploy-pages@v4
    
    - name: Summary
      if: always()
      run: |
        echo "## ðŸ›’ ALDI Scraper Summary (Final Update)" >> $GITHUB_STEP_SUMMARY
        echo "**Run completed:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.results.outputs.success }}" == "true" ]; then
          echo "### âœ… Success - Final Scraper Run!" >> $GITHUB_STEP_SUMMARY
          echo "- **Products scraped:** ${{ steps.results.outputs.product_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Store:** ALDI (Final update)" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** Data committed to repository" >> $GITHUB_STEP_SUMMARY
          echo "- **Website:** Rebuilt and deployed with ALL FRESH PRICES! ðŸŽ‰" >> $GITHUB_STEP_SUMMARY
          echo "- **Live URL:** ${{ steps.deployment.outputs.page_url }}" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸª All Stores Now Updated:" >> $GITHUB_STEP_SUMMARY
          echo "- **Sainsbury's:** Wednesday 12:00 AM UTC" >> $GITHUB_STEP_SUMMARY
          echo "- **Tesco:** Thursday 2:30 AM UTC" >> $GITHUB_STEP_SUMMARY
          echo "- **ALDI:** Thursday 7:00 AM UTC (FINAL - triggers website rebuild)" >> $GITHUB_STEP_SUMMARY
        else
          echo "### âŒ Scraping Failed" >> $GITHUB_STEP_SUMMARY
          echo "- **Error:** Failed to generate ALDI CSV file" >> $GITHUB_STEP_SUMMARY
          echo "- **Impact:** Website not rebuilt with latest prices" >> $GITHUB_STEP_SUMMARY
          echo "- **Action:** Check logs and retry if needed" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“‹ Technical Details" >> $GITHUB_STEP_SUMMARY
        echo "- **Final scraper run:** ALDI completes the weekly price update cycle" >> $GITHUB_STEP_SUMMARY
        echo "- **Website rebuild:** Triggered only after ALDI (last scraper)" >> $GITHUB_STEP_SUMMARY
        echo "- **Fresh data:** All three stores now have current prices" >> $GITHUB_STEP_SUMMARY
        echo "- **Next update:** Next Wednesday at midnight (Sainsbury's)" >> $GITHUB_STEP_SUMMARY