name: Morrisons Price Scraper

on:
  workflow_dispatch:
  schedule:
    - cron: '0 9 * * 1'  # Mondays at 9 AM UTC

permissions:
  contents: write

jobs:
  scrape-morrisons:
    runs-on: ubuntu-latest
    timeout-minutes: 60
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        token: ${{ secrets.GITHUB_TOKEN }}
        fetch-depth: 0
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y wget curl unzip xvfb libnss3-dev libgconf-2-4 libxss1 libappindicator1 libindicator7
    
    - name: Install Chrome
      run: |
        wget -q -O - https://dl-ssl.google.com/linux/linux_signing_key.pub | sudo apt-key add -
        echo "deb [arch=amd64] http://dl.google.com/linux/chrome/deb/ stable main" | sudo tee /etc/apt/sources.list.d/google-chrome.list
        sudo apt-get update
        sudo apt-get install -y google-chrome-stable
        google-chrome --version
    
    - name: Install ChromeDriver
      run: |
        CHROME_VERSION=$(google-chrome --version | sed 's/Google Chrome //' | cut -d'.' -f1)
        
        if [ "$CHROME_VERSION" -ge "115" ]; then
          LATEST_VERSION=$(curl -s "https://googlechromelabs.github.io/chrome-for-testing/LATEST_RELEASE_STABLE")
          wget -O /tmp/chromedriver.zip "https://storage.googleapis.com/chrome-for-testing-public/$LATEST_VERSION/linux64/chromedriver-linux64.zip"
          sudo unzip /tmp/chromedriver.zip -d /tmp/
          sudo mv /tmp/chromedriver-linux64/chromedriver /usr/local/bin/chromedriver
        else
          DRIVER_VERSION=$(curl -s "https://chromedriver.storage.googleapis.com/LATEST_RELEASE_${CHROME_VERSION}")
          wget -O /tmp/chromedriver.zip "https://chromedriver.storage.googleapis.com/${DRIVER_VERSION}/chromedriver_linux64.zip"
          sudo unzip /tmp/chromedriver.zip -d /usr/local/bin/
        fi
        
        sudo chmod +x /usr/local/bin/chromedriver
        chromedriver --version
    
    - name: Install Python dependencies
      run: |
        cd WebScrape
        pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Optimize Morrisons scraper for GitHub Actions
      run: |
        cd WebScrape
        cat > optimize_morrisons.py << 'EOF'
        import re
        
        # Read the morrisons.py file
        with open('morrisons.py', 'r') as f:
            content = f.read()
        
        print("Optimizing Morrisons scraper for GitHub Actions...")
        
        # Reduce max_workers for GitHub Actions stability (but keep all categories)
        content = re.sub(r'max_workers=\d+', 'max_workers=2', content)
        
        # Slightly reduce max_scrolls per category for faster execution but keep comprehensive
        content = re.sub(r'max_scrolls=\d+', 'max_scrolls=35', content)
        
        # Optimize sleep times for GitHub Actions
        content = content.replace('time.sleep(1.0)', 'time.sleep(0.8)')
        content = content.replace('time.sleep(1)', 'time.sleep(0.8)')
        
        # Don't remove any categories - keep all 11 categories for comprehensive data
        print("âœ… Keeping all 11 categories for comprehensive price data")
        
        # Write the optimized file
        with open('morrisons.py', 'w') as f:
            f.write(content)
        
        print("âœ… Morrisons scraper optimized for GitHub Actions")
        print("- Reduced workers to 2 for stability")
        print("- Reduced max scrolls to 35 per category")
        print("- Faster sleep intervals")
        print("- Keeping ALL 11 categories for complete data coverage")
        EOF
        
        python optimize_morrisons.py
    
    - name: Create public directory
      run: mkdir -p app/public
    
    - name: Run Morrisons scraper
      timeout-minutes: 55
      run: |
        cd WebScrape
        echo "ðŸ›’ Starting Morrisons scraper..."
        echo "ðŸ“‹ Categories to scrape: ALL 11 categories"
        echo "ðŸ• Estimated time: 45-55 minutes"
        
        # Run with virtual display and capture output
        xvfb-run -a -s "-screen 0 1920x1080x24" python morrisons.py > morrisons.log 2>&1
        
        echo "Morrisons scraper completed. Checking results..."
    
    - name: Check results and copy CSV
      id: results
      run: |
        cd WebScrape
        
        # Check if CSV was created
        if [ -f "morrisons.csv" ]; then
          # Copy to public directory
          cp morrisons.csv ../app/public/
          
          # Get product count
          total_lines=$(wc -l < morrisons.csv)
          product_count=$((total_lines - 1))  # Subtract header
          
          echo "âœ… Morrisons scraping successful!"
          echo "ðŸ“Š Products scraped: $product_count"
          echo "ðŸ“ CSV saved to both locations"
          
          # Output for later steps
          echo "success=true" >> $GITHUB_OUTPUT
          echo "product_count=$product_count" >> $GITHUB_OUTPUT
          
          # Show sample of results
          echo ""
          echo "ðŸ“‹ Sample products (first 5):"
          head -6 morrisons.csv
          
          # Show category breakdown
          echo ""
          echo "ðŸ“Š Products per category:"
          tail -n +2 morrisons.csv | cut -d',' -f3 | sort | uniq -c | sort -nr
          
        else
          echo "âŒ Morrisons scraping failed - no CSV generated"
          echo "ðŸ“‹ Last 30 lines of log:"
          tail -30 morrisons.log 2>/dev/null || echo "No log file found"
          
          echo "success=false" >> $GITHUB_OUTPUT
          echo "product_count=0" >> $GITHUB_OUTPUT
          exit 1
        fi
    
    - name: Upload scraper logs
      if: always()
      uses: actions/upload-artifact@v4
      with:
        name: morrisons-scraper-logs
        path: WebScrape/morrisons.log
        retention-days: 7
        if-no-files-found: ignore
    
    - name: Commit and push changes
      if: steps.results.outputs.success == 'true'
      run: |
        git config --global user.email "actions@github.com"
        git config --global user.name "GitHub Actions - Morrisons Bot"
        
        # Add the CSV file
        git add -f app/public/morrisons.csv
        
        # Check if there are changes to commit
        if ! git diff --staged --quiet; then
          echo "ðŸ“ Committing Morrisons price data..."
          
          git commit -m "ðŸ›’ Update Morrisons prices - $(date -u '+%Y-%m-%d %H:%M UTC')

          ðŸ“Š Products scraped: ${{ steps.results.outputs.product_count }}
          ðŸª Store: Morrisons
          ðŸ¤– Automated scraping via GitHub Actions
          
          ðŸ“‹ All 11 Categories Included:
          â€¢ Fruit, Veg & Flowers
          â€¢ Meat & Poultry  
          â€¢ Fish & Seafood
          â€¢ Fresh & Chilled Foods
          â€¢ Bakery & Cakes
          â€¢ Frozen Food
          â€¢ Food Cupboard
          â€¢ Dietary & Lifestyle Foods
          â€¢ World Foods
          â€¢ Drinks
          â€¢ Beer, Wines & Spirits"
          
          echo "ðŸ“¤ Pushing changes to repository..."
          git push origin HEAD:${{ github.ref_name }}
          echo "âœ… Morrisons data committed and pushed successfully!"
          
        else
          echo "â„¹ï¸ No changes detected - CSV content unchanged"
        fi
    
    - name: Create job summary
      if: always()
      run: |
        echo "## ðŸ›’ Morrisons Price Scraping Results" >> $GITHUB_STEP_SUMMARY
        echo "**Completed:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ "${{ steps.results.outputs.success }}" == "true" ]; then
          echo "### âœ… Scraping Successful" >> $GITHUB_STEP_SUMMARY
          echo "- **Products scraped:** ${{ steps.results.outputs.product_count }}" >> $GITHUB_STEP_SUMMARY
          echo "- **Categories:** All 11 main categories" >> $GITHUB_STEP_SUMMARY
          echo "- **File location:** \`app/public/morrisons.csv\`" >> $GITHUB_STEP_SUMMARY
          echo "- **Status:** Data committed to repository" >> $GITHUB_STEP_SUMMARY
          echo "" >> $GITHUB_STEP_SUMMARY
          echo "### ðŸ“Š Categories Scraped" >> $GITHUB_STEP_SUMMARY
          echo "1. Fruit, Veg & Flowers" >> $GITHUB_STEP_SUMMARY
          echo "2. Meat & Poultry" >> $GITHUB_STEP_SUMMARY
          echo "3. Fish & Seafood" >> $GITHUB_STEP_SUMMARY
          echo "4. Fresh & Chilled Foods" >> $GITHUB_STEP_SUMMARY
          echo "5. Bakery & Cakes" >> $GITHUB_STEP_SUMMARY
          echo "6. Frozen Food" >> $GITHUB_STEP_SUMMARY
          echo "7. Food Cupboard" >> $GITHUB_STEP_SUMMARY
          echo "8. Dietary & Lifestyle Foods" >> $GITHUB_STEP_SUMMARY
          echo "9. World Foods" >> $GITHUB_STEP_SUMMARY
          echo "10. Drinks" >> $GITHUB_STEP_SUMMARY
          echo "11. Beer, Wines & Spirits" >> $GITHUB_STEP_SUMMARY
        else
          echo "### âŒ Scraping Failed" >> $GITHUB_STEP_SUMMARY
          echo "- **Error:** Failed to generate CSV file" >> $GITHUB_STEP_SUMMARY
          echo "- **Logs:** Check uploaded artifacts for details" >> $GITHUB_STEP_SUMMARY
          echo "- **Next action:** Review logs and retry manually" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "### ðŸ“‹ Workflow Details" >> $GITHUB_STEP_SUMMARY
        echo "- **Trigger:** ${{ github.event_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Branch:** ${{ github.ref_name }}" >> $GITHUB_STEP_SUMMARY
        echo "- **Runner:** Ubuntu Latest" >> $GITHUB_STEP_SUMMARY
        echo "- **Timeout:** 60 minutes" >> $GITHUB_STEP_SUMMARY
        echo "- **Parallel workers:** 2" >> $GITHUB_STEP_SUMMARY